{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: copied and adapted from https://github.com/mkneierV/test_notebooks/blob/master/demo_data_prep.ipynb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "RUN_LOCAL = True\n",
    "ROOT_DIR = 'temp/papermill_demo'\n",
    "PROJECT = 'kubeflow-demo-256908'\n",
    "BUCKET = f'gs://{PROJECT}-dev'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: For using tf transform, ensure versions are compatible with: https://pypi.org/project/tensorflow-transform/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\"weight_pounds\", \"is_male\", \"mother_age\", \"mother_race\", \"plurality\", \"gestation_weeks\"]\n",
    "\n",
    "def get_source_query(step):\n",
    "    train_years = (1980,2004)\n",
    "    eval_years  = (2005,2007)\n",
    "    test_years  = (2008, 2008)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      weight_pounds,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      mother_race,\n",
    "      plurality,\n",
    "      gestation_weeks\n",
    "    FROM\n",
    "      publicdata.samples.natality\n",
    "    WHERE year BETWEEN {} AND {}\n",
    "      AND weight_pounds > 0\n",
    "      AND mother_age > 0\n",
    "      AND plurality > 0\n",
    "      AND gestation_weeks > 0\n",
    "      AND month > 0\n",
    "    LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    if step == 'eval':\n",
    "        source_query = query.format(*eval_years)\n",
    "    elif step == 'test':\n",
    "        source_query = query.format(*test_years)\n",
    "    elif step == \"train\":\n",
    "        source_query = query.format(*train_years)\n",
    "    else:\n",
    "        raise ValueError(\"step value of {} must be one of 'train', 'eval', 'test'\".format(step))\n",
    "    return source_query\n",
    "\n",
    "\n",
    "def read_from_bq(pipeline, step):\n",
    "    source_query = get_source_query(step)\n",
    "    raw_data = (\n",
    "        pipeline\n",
    "        | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(\n",
    "            beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "        | '{} - Clean up Data'.format(step) >> beam.Map(prep_bq_row)\n",
    "    )\n",
    "    \n",
    "    raw_metadata = create_raw_metadata()\n",
    "    raw_dataset = (raw_data, raw_metadata)\n",
    "    return raw_dataset\n",
    "\n",
    "\n",
    "def prep_bq_row(bq_row):\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = {} \n",
    "    \n",
    "    for feature_name in bq_row.keys():\n",
    "        result[feature_name] = str(bq_row[feature_name])\n",
    "\n",
    "    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n",
    "        result['mother_race'] = races[bq_row['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def to_csv_string(bq_dict):\n",
    "    FEATURES = [\"weight_pounds\", \"is_male\", \"mother_age\", \"mother_race\", \"plurality\", \"gestation_weeks\"]\n",
    "    output = []\n",
    "    for f in FEATURES:\n",
    "        output.append(bq_dict[f])\n",
    "        \n",
    "    return \",\".join(output)\n",
    "\n",
    "\n",
    "def write_csv(transformed_data, location, step): \n",
    "    FEATURES = [\"weight_pounds\", \"is_male\", \"mother_age\", \"mother_race\", \"plurality\", \"gestation_weeks\"]\n",
    "    (\n",
    "        transformed_data \n",
    "        | '{} - Write Transformed Data'.format(step) >> beam.io.WriteToText(\n",
    "            file_path_prefix=os.path.join(location, step, step),\n",
    "            file_name_suffix=\".csv\",\n",
    "            header=\",\".join(FEATURES)\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformation_pipeline(args):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "    \n",
    "    runner = args['runner']\n",
    "    transformed_data_location = args['transformed_data_location']\n",
    "    transform_artifact_location = args['transform_artifact_location']\n",
    "    temporary_dir = args['temporary_dir']\n",
    "    \n",
    "    print(\"Sink transformed data files location: {}\".format(transformed_data_location))\n",
    "    print(\"Sink transform artifact location: {}\".format(transform_artifact_location))\n",
    "    print(\"Temporary directory: {}\".format(temporary_dir))\n",
    "    print(\"Runner: {}\".format(runner))\n",
    "\n",
    "    with beam.Pipeline(runner, options=pipeline_options) as pipeline:            \n",
    "        for step in (\"train\", \"eval\", \"test\"):\n",
    "            source_query = get_source_query(step)\n",
    "            print('query built')\n",
    "            data = (\n",
    "                pipeline\n",
    "                | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(\n",
    "                    beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "                | '{} - Clean up Data'.format(step) >> beam.Map(prep_bq_row)\n",
    "                | '{} - Prepare for csv'.format(step) >> beam.Map(to_csv_string)\n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.WriteToText(\n",
    "                    file_path_prefix=os.path.join(transformed_data_location, step, step),\n",
    "                    file_name_suffix=\".csv\")\n",
    "            )\n",
    "            # Write transformed train data to sink in csv\n",
    "            # write_csv(data, transformed_data_location, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = os.path.join(BUCKET, ROOT_DIR)\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(OUTPUT_DIR,'transform')\n",
    "TRANSFORMED_DATA_DIR = os.path.join(OUTPUT_DIR,'transformed')\n",
    "TEMP_DIR = os.path.join(OUTPUT_DIR, 'tmp')\n",
    "\n",
    "runner = 'DirectRunner' if RUN_LOCAL == True else 'DataflowRunner'\n",
    "\n",
    "job_name = 'preprocess-babweight-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "\n",
    "args = {\n",
    "    'job_name': job_name,\n",
    "    'runner': runner,\n",
    "    'transformed_data_location':  TRANSFORMED_DATA_DIR,\n",
    "    'transform_artifact_location':  TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': TEMP_DIR,\n",
    "    'debug':False,\n",
    "    \n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'worker_machine_type': 'n1-standard-1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_transformation_pipeline(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {TRANSFORMED_DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(\n",
    "    os.path.join(TRANSFORMED_DATA_DIR, 'eval', 'eval-00000-of-00001.csv'),\n",
    "    names=[\n",
    "        'weight_pounds',\n",
    "        'is_male',\n",
    "        'mother_age',\n",
    "        'mother_race',\n",
    "        'plurality',\n",
    "        'gestation_weeks'\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
